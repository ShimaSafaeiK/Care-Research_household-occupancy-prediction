{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Project Overview**\n",
        "\n",
        "project aims to predict whether a household is single or multiple occupancy using motion sensor data. This involves:\n",
        "\n",
        "1.Loading and cleaning the data.\n",
        "\n",
        "2.Merging sensor data with occupancy labels.\n",
        "\n",
        "3.Preprocessing and feature engineering.\n",
        "\n",
        "4.Building and evaluating a predictive model.\n",
        "\n",
        "5.Optionally, exporting the model to ONNX format for deployment.**"
      ],
      "metadata": {
        "id": "l7uUF2ocqqyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Additional Explanation**\n",
        "\n",
        "***Data Understanding:*** The homes.csv file contains information about each household, specifically whether it is single or multiple occupancy. The motion.csv file contains timestamps and locations of motion detected in different homes.\n",
        "\n",
        "***Data Cleaning:*** Cleaning column names is crucial for avoiding errors in subsequent steps and ensuring that the data is consistently formatted.\n",
        "\n",
        "***Merging Data:*** Merging the two datasets allows us to combine sensor readings with occupancy labels, enabling us to create features that link motion patterns to occupancy types."
      ],
      "metadata": {
        "id": "QfzzzIDpr3yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Import Libraries and Load Data\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV files\n",
        "homes_df = pd.read_csv('/content/homes.csv')\n",
        "motion_df = pd.read_csv('/content/motion.csv')\n",
        "\n",
        "# Display the first few rows to ensure they are loaded correctly\n",
        "print(\"Homes Data:\")\n",
        "print(homes_df.head())\n",
        "\n",
        "print(\"\\nMotion Data:\")\n",
        "print(motion_df.head())\n",
        "\n",
        "# Step 2: Clean Column Names\n",
        "# Remove any leading/trailing whitespace and double quotes from column names in both DataFrames.\n",
        "# This ensures consistency and prevents issues during merging and data manipulation.\n",
        "\n",
        "homes_df.columns = homes_df.columns.str.strip().str.replace('\"', '')\n",
        "motion_df.columns = motion_df.columns.str.strip().str.replace('\"', '')\n",
        "\n",
        "# Step 3: Merge DataFrames\n",
        "# Merge the motion table with the homes table based on the 'id' column\n",
        "merged_df = pd.merge(motion_df, homes_df, left_on='home_id', right_on='id', how='left')\n",
        "\n",
        "# Verify columns before dropping any\n",
        "print(\"Columns in merged DataFrame before dropping:\")\n",
        "print(merged_df.columns)\n",
        "\n",
        "# Drop the redundant 'id' column from homes_df after merge\n",
        "merged_df.drop(columns='id_y', inplace=True)\n",
        "\n",
        "# Rename columns for clarity if necessary\n",
        "merged_df.rename(columns={'id_x': 'motion_id', 'id': 'home_id'}, inplace=True)\n",
        "\n",
        "# Display the merged DataFrame to check\n",
        "print(\"Merged DataFrame:\")\n",
        "print(merged_df.head())"
      ],
      "metadata": {
        "id": "bceFqg_x6pom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*     **With these steps, the data is now ready for further preprocessing and feature engineering, which are critical for building a predictive model.**"
      ],
      "metadata": {
        "id": "RdLhzm-TsYnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Additional Explanation:**\n",
        "\n",
        "***Datetime Conversion:*** Converting the 'datetime' column to a datetime type allows for the extraction of specific time-related features (e.g., hour, day of the week), which can be crucial for understanding patterns in sensor data.\n",
        "\n",
        "***Feature Extraction:*** Extracting features like hour and day_of_week helps in capturing temporal patterns in sensor activations, which can be indicative of occupancy behavior.\n",
        "\n",
        "***Aggregation:*** Aggregating data by day, Home_Id, and location allows for summarizing sensor activities over a day, which can reveal patterns that distinguish between single and multiple occupancy households.\n",
        "\n",
        "***Merging and Cleaning:*** Merging with the target variable and cleaning the DataFrame ensures that the data is ready for model training, with all necessary features and the target variable included."
      ],
      "metadata": {
        "id": "FIBiMXIPuExb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Group By and Aggregate:**\n",
        "Group the merged_df DataFrame by date, home ID, and location. Aggregate the following features:\n",
        "\n",
        "**motion_id:** Count the number of unique sensor events (nunique).\n",
        "\n",
        "**datetime:** Count the total number of events (count).\n",
        "\n",
        "**hour:** Calculate the mean and standard deviation of the hour (mean, std).\n",
        "\n",
        "**day_of_week:** Calculate the mean day of the week (mean).\n",
        "\n",
        "\n",
        "**Reset Index:** Reset the index of the aggregated DataFrame to convert grouped indices into columns.\n",
        "\n",
        "**Rename Columns: Rename the columns for clarity:**\n",
        "\n",
        "**date:** The date of the events.\n",
        "\n",
        "**home_id:** The ID of the home.\n",
        "\n",
        "**location:** The location of the sensor.\n",
        "\n",
        "**unique_sensors:** The number of unique sensors triggered.\n",
        "\n",
        "**total_events:** The total number of sensor events.( events when motion happens)\n",
        "\n",
        "**avg_hour:** The average hour of the events.\n",
        "\n",
        "**std_hour:** The standard deviation of the hour of events.\n",
        "\n",
        "**avg_day_of_week:** The average day of the week of events."
      ],
      "metadata": {
        "id": "QuRNXASsvN0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Convert 'datetime' Column to Datetime Type\n",
        "merged_df['datetime'] = pd.to_datetime(merged_df['datetime'])\n",
        "\n",
        "\n",
        "# Step 5: Extract Additional Time-Based Features\n",
        "merged_df['hour'] = merged_df['datetime'].dt.hour\n",
        "merged_df['day_of_week'] = merged_df['datetime'].dt.dayofweek\n",
        "\n",
        "\n",
        "# Step 6: Aggregate Features by Day and Home ID\n",
        "daily_features = merged_df.groupby([merged_df['datetime'].dt.date, 'home_id', 'location']).agg({\n",
        "    'motion_id': 'nunique',  # Number of unique sensor events\n",
        "    'datetime': 'count',  # Total number of events\n",
        "    'hour': ['mean', 'std'],  # Mean and std of hour\n",
        "    'day_of_week': 'mean'  # Mean day of week\n",
        "}).reset_index()\n",
        "\n",
        " # Rename columns\n",
        "daily_features.columns = ['date', 'home_id','location', 'unique_sensors', 'total_events', 'avg_hour', 'std_hour', 'avg_day_of_week']\n",
        "\n",
        "# Step 7: Add Target Variable\n",
        "data = pd.merge(daily_features, homes_df[['id', 'multiple_occupancy']], left_on='home_id', right_on='id', how='left')\n",
        "\n",
        "# Step 8: Clean Up DataFrame\n",
        "data.drop(columns=['id'], inplace=True)\n",
        "\n",
        "# Display the final DataFrame with features and target\n",
        "print(\"Final Data with Features and Target:\")\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "m86oxt_e6pd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "5kDn_ZIt6pDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = data[['location','total_events','avg_hour','std_hour','avg_day_of_week','multiple_occupancy']]\n",
        "\n",
        "# Display the dataframe\n",
        "print(df.head(5))"
      ],
      "metadata": {
        "id": "oJP8Xw7BwXqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1=df.copy()\n",
        "df1['location'] = pd.Categorical(df1['location']).codes\n",
        "print(df1.head(5))\n"
      ],
      "metadata": {
        "id": "PqpDUV9v8WB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The code below in step 7 : Prepare Data for Each Fold***\n",
        "\n",
        "**Loop Over Folds:** For each fold, split data into training, validation, and test sets.\n",
        "\n",
        "**Bootstrapping:** Balance classes in the training set by resampling.\n",
        "\n",
        "**Normalize Data:** Fit the scaler on the training data and transform the data for training, validation, and test sets.\n",
        "\n",
        "**Store Scaled Data:** Append the scaled data and labels to respective lists."
      ],
      "metadata": {
        "id": "hIk3dM9E3SPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "This part of the code below does the following:\n",
        "\n",
        "**Prepares the Data:** Separates features and labels, and sets up cross-validation splits.\n",
        "\n",
        "**Balances the Training Data:** Uses bootstrapping to balance class distributions.\n",
        "\n",
        "**Normalizes the Data:** Scales the features using MinMaxScaler.\n",
        "\n",
        "**Hyperparameter Tuning:** Uses GridSearchCV to find the best hyperparameters for the XGBoost model.\n",
        "\n",
        "**Evaluates the Model:** Evaluates the performance of the best models on test sets and calculates the average test accuracy."
      ],
      "metadata": {
        "id": "_GRDTh2j5Wp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold, GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.utils import resample\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "# Step 2: Prepare Data\n",
        "# X: Contains all columns except the last one (features).\n",
        "# y: Contains only the last column (target variable).\n",
        "X = df1.iloc[:, :-1]\n",
        "y = df1.iloc[:, -1]\n",
        "\n",
        "# Step 3: Initialize KFold\n",
        "# KFold: Initializes K-Folds cross-validation with 2 splits.\n",
        "# Lists: To store indices of training, validation, and test splits.\n",
        "kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
        "\n",
        "# Lists to store the splits\n",
        "train_indices = []\n",
        "val_indices = []\n",
        "test_indices = []\n",
        "\n",
        "# Step 4: Generate Splits\n",
        "# Outer Loop: Splits data into training and test sets.\n",
        "# Inner Loop: Further splits the training set into training and validation sets.\n",
        "for train_index, test_index in kf.split(X):\n",
        "    train_indices.append(train_index)\n",
        "    test_indices.append(test_index)\n",
        "    # Use part of the training data for validation\n",
        "    sub_kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
        "    for sub_train_index, val_index in sub_kf.split(train_index):\n",
        "        val_indices.append(val_index)\n",
        "        break  # Only need one validation set per outer split\n",
        "\n",
        "# Step 5: Normalize the data using MinMaxScaler based on the training set only\n",
        "# MinMaxScaler: Scales features to a range between 0 and 1.\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Step 6: For storing scaled data for training, validation, and test sets.\n",
        "X_train_scaled = []\n",
        "X_val_scaled = []\n",
        "X_test_scaled = []\n",
        "y_train_list = []\n",
        "y_val_list = []\n",
        "y_test_list = []\n",
        "\n",
        "# Step 7: Prepare Data for Each Fold\n",
        "for i in range(2):\n",
        "    # Split the data\n",
        "    X_train, X_val, X_test = X.iloc[train_indices[i]], X.iloc[val_indices[i]], X.iloc[test_indices[i]]\n",
        "    y_train, y_val, y_test = y.iloc[train_indices[i]], y.iloc[val_indices[i]], y.iloc[test_indices[i]]\n",
        "\n",
        "    # Combine the training data\n",
        "    train_data = pd.concat([X_train, y_train], axis=1)\n",
        "\n",
        "    # Perform bootstrapping to balance the classes\n",
        "    max_size = train_data[y.name].value_counts().max()\n",
        "    lst = [train_data]\n",
        "    for class_index, group in train_data.groupby(y.name):\n",
        "        lst.append(group.sample(max_size-len(group), replace=True))\n",
        "    train_data_balanced = pd.concat(lst)\n",
        "\n",
        "    # Separate the predictors and labels again\n",
        "    X_train_balanced = train_data_balanced.iloc[:, :-1]\n",
        "    y_train_balanced = train_data_balanced.iloc[:, -1]\n",
        "\n",
        "    # Fit the scaler on the balanced training data\n",
        "    scaler.fit(X_train_balanced)\n",
        "\n",
        "    # Transform the data\n",
        "    X_train_scaled.append(scaler.transform(X_train_balanced))\n",
        "    X_val_scaled.append(scaler.transform(X_val))\n",
        "    X_test_scaled.append(scaler.transform(X_test))\n",
        "\n",
        "    y_train_list.append(y_train_balanced)\n",
        "    y_val_list.append(y_val)\n",
        "    y_test_list.append(y_test)\n",
        "\n",
        "# Now we have 5 sets of (X_train_scaled, y_train), (X_val_scaled, y_val), (X_test_scaled, y_test)\n",
        "\n",
        "#Step 8: Initialize and Define XGBoost Classifier\n",
        "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "\n",
        "# Define the parameter grid for GridSearchCV,Define a grid of hyperparameters for tuning.\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "# step9: Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy',cv=kf)\n",
        "\n",
        "# Step 10: Perform Grid Search and Evaluate Models\n",
        "best_models = []\n",
        "for i in range(2):\n",
        "    X_train, y_train = X_train_scaled[i], y_train_list[i]\n",
        "    X_val, y_val = X_val_scaled[i], y_val_list[i]\n",
        "    X_test, y_test = X_test_scaled[i], y_test_list[i]\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_models.append(grid_search.best_estimator_)\n",
        "    print(f\"Best params for fold {i}: {grid_search.best_params_}\")\n",
        "    print(f\"Best score for fold {i}: {grid_search.best_score_}\")\n",
        "\n",
        "# Step 11: Evaluate Best Models on Test Sets\n",
        "test_accuracies = []\n",
        "for i, best_model in enumerate(best_models):\n",
        "    X_test, y_test = X_test_scaled[i], y_test_list[i]\n",
        "    test_accuracy = best_model.score(X_test, y_test)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    print(f\"Test accuracy for fold {i}: {test_accuracy}\")\n",
        "\n",
        "print(f\"Average test accuracy: {sum(test_accuracies) / len(test_accuracies)}\")\n"
      ],
      "metadata": {
        "id": "Bznx0tL1ybU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Suppose you have the following accuracy scores for two test sets:**\n",
        "\n",
        "**Accuracy for Fold 0: 0.85**\n",
        "\n",
        "**Accuracy for Fold 1: 0.9**\n",
        "\n",
        "The average test accuracy would be calculated as:\n",
        "\n",
        "**Average test accuracy. =( 0.85 + 0.90 / 2 )= 0.875**"
      ],
      "metadata": {
        "id": "qkxfYCguALGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Best params for fold 1: {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 300, 'subsample': 0.8}**\n",
        "\n",
        "**Best score for fold 1: 0.7609078889378581**\n",
        "\n",
        "**Test accuracy for fold 0: 0.7075417386298215**\n",
        "\n",
        "**Test accuracy for fold 1: 0.7037143679815722**\n",
        "\n",
        "**Average test accuracy: 0.7056280533056969**\n"
      ],
      "metadata": {
        "id": "62fjtywGhEnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average test accuracy: {sum(test_accuracies) / len(test_accuracies)}\")"
      ],
      "metadata": {
        "id": "0q56MgExGb1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assuming `best_models` is a list of the best models from each fold\n",
        "# and `X_test_scaled` and `y_test_list` are lists of test data for each fold\n",
        "\n",
        "# Initialize lists to store F1 scores\n",
        "f1_scores = []\n",
        "\n",
        "# Evaluate each best model on the corresponding test set\n",
        "for i, best_model in enumerate(best_models):\n",
        "    X_test = X_test_scaled[i]\n",
        "    y_test = y_test_list[i]\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Calculate F1 score (using 'weighted' average to handle class imbalance)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Print F1 score for the current fold\n",
        "    print(f\"F1 Score for fold {i}: {f1}\")\n",
        "\n",
        "    # Store F1 score\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "# Calculate and print the average F1 score across all folds\n",
        "average_f1_score = sum(f1_scores) / len(f1_scores)\n",
        "print(f\"Average F1 Score: {average_f1_score}\")\n"
      ],
      "metadata": {
        "id": "Bk_7B961iFmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "During the development of our household occupancy prediction model, we used an ensemble model called **XGBoost** . We tested different combinations of settings (called **hyperparameters**) to find the ones that provided the most accurate predictions, using Gridsearch.\n",
        "\n",
        "For our project, we ran a series of tests (called folds) to ensure our model was robust. In the first test **(fold 0)**, the best model used all available features. In the second test **(fold 1)**, the best model used 80% of the features. Despite this difference, both models agreed on key settings: they both had a learning rate of 0.2 and a maximum depth of 5.\n",
        "\n",
        "\n",
        "**Relevance to the Project**\n",
        "\n",
        "Our project aims to predict whether a household has single or multiple occupants based on data from motion sensors installed in various rooms. The models we identified as the best are crucial because they optimize our ability to make these predictions accurately. By using these models, we can reliably determine the occupancy status of a household, which can be valuable for various applications such as smart home management, and elder care.\n",
        "\n",
        "By carefully selecting and validating these models, we ensure that our predictions are not only accurate but also reliable when applied to new, unseen data. This process is essential to build trust in our system's ability to monitor and analyze household occupancy effectively."
      ],
      "metadata": {
        "id": "b33uND2y_gkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_models[0]"
      ],
      "metadata": {
        "id": "JfiuV7LXKaoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_models[1]"
      ],
      "metadata": {
        "id": "KP--XIe-Gx9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "2EhhnFNt15c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " To interpret the predictions of our XGBoost models, we employed **SHAP** (SHapley Additive exPlanations), a powerful tool for model interpretability. SHAP provides insights into how each feature influences the model’s predictions, helping us understand the model's decision-making process."
      ],
      "metadata": {
        "id": "HkD6DR9hIb2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary code below :**\n",
        "This analysis helps to understand the impact of each feature on the model’s predictions by visualizing:\n",
        "\n",
        "**Individual Prediction Impact:** How features contribute to the prediction of specific instances.\n",
        "\n",
        "**Overall Feature Importance:** How features generally affect the model’s decisions across the dataset.\n",
        "\n",
        "**Aggregated Insights:** A consolidated view of feature contributions from all cross-validation folds.\n",
        "\n",
        "Using SHAP, you gain deeper insights into our model’s behavior, enhancing interpretability and trust in the predictions."
      ],
      "metadata": {
        "id": "2m5rcT0lJOFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SHAP Analysis per Fold:\n",
        "# For each fold of our cross-validation, we initialized a SHAP explainer using the best model from the GridSearchCV.\n",
        "# This explainer calculates SHAP values,which quantify the contribution of each feature to the model’s predictions for the test data.\n",
        "\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# Assuming the best models from the previous step are stored in `best_models`\n",
        "# and df is your original DataFrame\n",
        "\n",
        "# Extract real feature names from the DataFrame\n",
        "feature_names = df1.columns[:-1].tolist()  # Exclude the label column\n",
        "\n",
        "# To store combined SHAP values and corresponding test data\n",
        "all_shap_values = []\n",
        "all_X_test = []\n",
        "\n",
        "# Iterate through each fold\n",
        "for i in range(2):\n",
        "    best_model = best_models[i]\n",
        "    X_test = X_test_scaled[i]\n",
        "    y_test = y_test_list[i]\n",
        "\n",
        "    # Initialize the SHAP explainer for the current fold's best model\n",
        "    explainer = shap.Explainer(best_model)\n",
        "\n",
        "    # Calculate SHAP values for the test data\n",
        "    shap_values = explainer(X_test)\n",
        "\n",
        "    # Store SHAP values and test data\n",
        "    all_shap_values.append(shap_values.values)\n",
        "    all_X_test.append(X_test)\n",
        "\n",
        "    # Visualize the SHAP values for the first prediction in the test set\n",
        "    print(f\"SHAP analysis for fold {i+1}\")\n",
        "    shap.waterfall_plot(shap_values[0])\n",
        "    shap.summary_plot(shap_values, X_test, feature_names=feature_names)\n",
        "\n",
        "# Combine all SHAP values and test data\n",
        "combined_shap_values = np.concatenate(all_shap_values, axis=0)\n",
        "combined_X_test = np.concatenate(all_X_test, axis=0)\n",
        "\n",
        "# Summary plot for the aggregated SHAP values\n",
        "shap.summary_plot(combined_shap_values, combined_X_test, feature_names=feature_names)\n"
      ],
      "metadata": {
        "id": "rJ0KnD_703ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP Analysis Summary\n",
        "**1. Feature Impacts on Prediction**\n",
        "\n",
        "**Feature 3:**\n",
        "Most significant positive impact, boosting the model’s output.\n",
        "\n",
        "**Feature 4:**\n",
        "Only feature with a negative impact, decreasing the final prediction value.\n",
        "\n",
        "**2. Combined Feature Effect**\n",
        "\n",
        "Final prediction value: 4.017.\n",
        "Result of all features' combined positive and negative influences.\n",
        "\n",
        "**3. SHAP Value Interpretation**\n",
        "\n",
        "**Positive SHAP Values:**\n",
        "Dots on the right show features increasing the model's output.\n",
        "\n",
        "**Negative SHAP Values:**\n",
        "Dots on the left show features decreasing the model's output.\n",
        "\n",
        "**4. Example Insight**\n",
        "\n",
        "Feature Values:\n",
        "Red dots indicate high feature values; blue dots indicate low values.\n",
        "For instance, many red dots for total_events suggest higher values boost the model’s output.\n",
        "\n",
        "**5. SHAP Waterfall Plot**\n",
        "\n",
        "Base Value: 0.027.\n",
        "Feature Impacts:\n",
        "Feature 1: Largest negative impact, reducing output by 0.98.\n",
        "Feature 2: Significant negative impact, reducing output by 0.75."
      ],
      "metadata": {
        "id": "J1wVymswL4UW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# Assuming the best models from the previous step are stored in `best_models`\n",
        "# and df is your original DataFrame\n",
        "\n",
        "# Extract real feature names from the DataFrame\n",
        "feature_names = df1.columns[:-1].tolist()  # Exclude the label column\n",
        "\n",
        "# To store combined SHAP values and corresponding data\n",
        "all_shap_values = []\n",
        "all_combined_data = []\n",
        "\n",
        "# Iterate through each fold\n",
        "for i in range(2):\n",
        "    best_model = best_models[i]\n",
        "    X_train = X_train_scaled[i]\n",
        "    X_val = X_val_scaled[i]\n",
        "    X_test = X_test_scaled[i]\n",
        "\n",
        "    # Combine training, validation, and test data\n",
        "    X_combined = np.vstack((X_train, X_val, X_test))\n",
        "\n",
        "    # Initialize the SHAP explainer for the current fold's best model\n",
        "    explainer = shap.Explainer(best_model)\n",
        "\n",
        "    # Calculate SHAP values for the combined data\n",
        "    shap_values = explainer(X_combined)\n",
        "\n",
        "    # Store SHAP values and combined data\n",
        "    all_shap_values.append(shap_values.values)\n",
        "    all_combined_data.append(X_combined)\n",
        "\n",
        "    # Visualize the SHAP values for the first prediction in the combined set\n",
        "    print(f\"SHAP analysis for fold {i+1}\")\n",
        "    shap.waterfall_plot(shap_values[0])\n",
        "    shap.summary_plot(shap_values, X_combined, feature_names=feature_names)\n",
        "\n",
        "# Combine all SHAP values and combined data\n",
        "combined_shap_values = np.concatenate(all_shap_values, axis=0)\n",
        "combined_data = np.concatenate(all_combined_data, axis=0)\n",
        "\n",
        "# Summary plot for the aggregated SHAP values\n",
        "shap.summary_plot(combined_shap_values, combined_data, feature_names=feature_names)\n"
      ],
      "metadata": {
        "id": "A0KKQLtN3Od5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}